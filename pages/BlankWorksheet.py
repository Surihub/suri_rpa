import streamlit as st
from kiwipiepy import Kiwi
import docx
from io import BytesIO
import numpy as np

# from transformers import BertTokenizer, BertModel
# import torch

# í˜ì´ì§€ ì œëª© ì„¤ì •
st.title("ğŸ“Create Blank Worksheet!")

# ì‚¬ìš©ë²• ì•ˆë‚´
st.info('###### ì‚¬ìš©ë²•\në¹ˆì¹¸ì„ ë§Œë“¤ì–´ì„œ ê³µë¶€í•˜ë ¤ê³  í•  ë•Œ, ì›í•˜ëŠ” í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥ í›„ íŠ¹ì • ë‹¨ì–´ë¥¼ ì„ íƒí•´ì„œ ë¹ˆì¹¸ì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.')

# ë¹ˆì¹¸ì„ ë§Œë“¤ ë‚´ìš©ì„ ì…ë ¥ë°›ìŒ
sample_text = """ì œ1ì¡° â‘ ëŒ€í•œë¯¼êµ­ì€ ë¯¼ì£¼ê³µí™”êµ­ì´ë‹¤.
â‘¡ëŒ€í•œë¯¼êµ­ì˜ ì£¼ê¶Œì€ êµ­ë¯¼ì—ê²Œ ìˆê³ , ëª¨ë“  ê¶Œë ¥ì€ êµ­ë¯¼ìœ¼ë¡œë¶€í„° ë‚˜ì˜¨ë‹¤.
 ì œ2ì¡° â‘ ëŒ€í•œë¯¼êµ­ì˜ êµ­ë¯¼ì´ ë˜ëŠ” ìš”ê±´ì€ ë²•ë¥ ë¡œ ì •í•œë‹¤.
â‘¡êµ­ê°€ëŠ” ë²•ë¥ ì´ ì •í•˜ëŠ” ë°”ì— ì˜í•˜ì—¬ ì¬ì™¸êµ­ë¯¼ì„ ë³´í˜¸í•  ì˜ë¬´ë¥¼ ì§„ë‹¤.
 ì œ3ì¡° ëŒ€í•œë¯¼êµ­ì˜ ì˜í† ëŠ” í•œë°˜ë„ì™€ ê·¸ ë¶€ì†ë„ì„œë¡œ í•œë‹¤.
 ì œ4ì¡° ëŒ€í•œë¯¼êµ­ì€ í†µì¼ì„ ì§€í–¥í•˜ë©°, ììœ ë¯¼ì£¼ì  ê¸°ë³¸ì§ˆì„œì— ì…ê°í•œ í‰í™”ì  í†µì¼ì •ì±…ì„ ìˆ˜ë¦½í•˜ê³  ì´ë¥¼ ì¶”ì§„í•œë‹¤.
 ì œ5ì¡° â‘ ëŒ€í•œë¯¼êµ­ì€ êµ­ì œí‰í™”ì˜ ìœ ì§€ì— ë…¸ë ¥í•˜ê³  ì¹¨ëµì  ì „ìŸì„ ë¶€ì¸í•œë‹¤.
â‘¡êµ­êµ°ì€ êµ­ê°€ì˜ ì•ˆì „ë³´ì¥ê³¼ êµ­í† ë°©ìœ„ì˜ ì‹ ì„±í•œ ì˜ë¬´ë¥¼ ìˆ˜í–‰í•¨ì„ ì‚¬ëª…ìœ¼ë¡œ í•˜ë©°, ê·¸ ì •ì¹˜ì  ì¤‘ë¦½ì„±ì€ ì¤€ìˆ˜ëœë‹¤.
 ì œ6ì¡° â‘ í—Œë²•ì— ì˜í•˜ì—¬ ì²´ê²°ã†ê³µí¬ëœ ì¡°ì•½ê³¼ ì¼ë°˜ì ìœ¼ë¡œ ìŠ¹ì¸ëœ êµ­ì œë²•ê·œëŠ” êµ­ë‚´ë²•ê³¼ ê°™ì€ íš¨ë ¥ì„ ê°€ì§„ë‹¤.
â‘¡ì™¸êµ­ì¸ì€ êµ­ì œë²•ê³¼ ì¡°ì•½ì´ ì •í•˜ëŠ” ë°”ì— ì˜í•˜ì—¬ ê·¸ ì§€ìœ„ê°€ ë³´ì¥ëœë‹¤.
 ì œ7ì¡° â‘ ê³µë¬´ì›ì€ êµ­ë¯¼ì „ì²´ì— ëŒ€í•œ ë´‰ì‚¬ìì´ë©°, êµ­ë¯¼ì— ëŒ€í•˜ì—¬ ì±…ì„ì„ ì§„ë‹¤.
â‘¡ê³µë¬´ì›ì˜ ì‹ ë¶„ê³¼ ì •ì¹˜ì  ì¤‘ë¦½ì„±ì€ ë²•ë¥ ì´ ì •í•˜ëŠ” ë°”ì— ì˜í•˜ì—¬ ë³´ì¥ëœë‹¤.
 ì œ8ì¡° â‘ ì •ë‹¹ì˜ ì„¤ë¦½ì€ ììœ ì´ë©°, ë³µìˆ˜ì •ë‹¹ì œëŠ” ë³´ì¥ëœë‹¤.
â‘¡ì •ë‹¹ì€ ê·¸ ëª©ì ã†ì¡°ì§ê³¼ í™œë™ì´ ë¯¼ì£¼ì ì´ì–´ì•¼ í•˜ë©°, êµ­ë¯¼ì˜ ì •ì¹˜ì  ì˜ì‚¬í˜•ì„±ì— ì°¸ì—¬í•˜ëŠ”ë° í•„ìš”í•œ ì¡°ì§ì„ ê°€ì ¸ì•¼ í•œë‹¤.
â‘¢ì •ë‹¹ì€ ë²•ë¥ ì´ ì •í•˜ëŠ” ë°”ì— ì˜í•˜ì—¬ êµ­ê°€ì˜ ë³´í˜¸ë¥¼ ë°›ìœ¼ë©°, êµ­ê°€ëŠ” ë²•ë¥ ì´ ì •í•˜ëŠ” ë°”ì— ì˜í•˜ì—¬ ì •ë‹¹ìš´ì˜ì— í•„ìš”í•œ ìê¸ˆì„ ë³´ì¡°í•  ìˆ˜ ìˆë‹¤.
â‘£ì •ë‹¹ì˜ ëª©ì ì´ë‚˜ í™œë™ì´ ë¯¼ì£¼ì  ê¸°ë³¸ì§ˆì„œì— ìœ„ë°°ë  ë•Œì—ëŠ” ì •ë¶€ëŠ” í—Œë²•ì¬íŒì†Œì— ê·¸ í•´ì‚°ì„ ì œì†Œí•  ìˆ˜ ìˆê³ , ì •ë‹¹ì€ í—Œë²•ì¬íŒì†Œì˜ ì‹¬íŒì— ì˜í•˜ì—¬ í•´ì‚°ëœë‹¤.
 ì œ9ì¡° êµ­ê°€ëŠ” ì „í†µë¬¸í™”ì˜ ê³„ìŠ¹ã†ë°œì „ê³¼ ë¯¼ì¡±ë¬¸í™”ì˜ ì°½ë‹¬ì— ë…¸ë ¥í•˜ì—¬ì•¼ í•œë‹¤."""

contents = st.text_area("ë¹ˆì¹¸ì„ ë§Œë“¤ ë‚´ìš©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.", value=sample_text)

@st.cache_data
def show_download():
        # 'ìƒì„±í•˜ê¸°' ë²„íŠ¼
    if st.button('ë¹ˆì¹¸ ëš«ê¸° ë¯¸ë¦¬ë³´ê¸°'):
        # ì²´í¬ëœ ëª…ì‚¬ë¥¼ ë¹ˆì¹¸ìœ¼ë¡œ ì¹˜í™˜
        for noun in selected_nouns:
            contents = contents.replace(noun, '___'*len(noun))

        # ê²°ê³¼ í‘œì‹œ
        st.write(contents)

        # ì›Œë“œ ë¬¸ì„œ ìƒì„±
        doc = docx.Document()
        doc.add_paragraph(contents)
        
        # ì›Œë“œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        with BytesIO() as f:
            doc.save(f)
            f.seek(0)
            st.download_button('ì›Œë“œ íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œí•˜ê¸°', f, file_name='í•™ìŠµì§€.docx')


tab1, tab2, tab3 = st.tabs(['ë¬¸ì¥ë³„ë¡œ í‚¤ì›Œë“œ ì„ íƒí•˜ê¸°', 'ìˆ˜ì‘ì—…[ê°€ë‚˜ë‹¤ìˆœ]', 'ì¶”ì²œ í‚¤ì›Œë“œ'])

import pandas as pd
with tab1:

    kiwi = Kiwi()
    # í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
    sentences = kiwi.split_into_sents(contents)
    sentences = pd.DataFrame(sentences).text.tolist()

    # ì„ íƒëœ ëª…ì‚¬ë¥¼ ì €ì¥í•˜ëŠ” ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if 'selected_nouns_dict' not in st.session_state:
        st.session_state.selected_nouns_dict = {}

    new_sentences = []

    for i, sentence in enumerate(sentences):
        # í˜•íƒœì†Œ ë¶„ì„ìœ¼ë¡œ ëª…ì‚¬ ì¶”ì¶œ
        tokens = kiwi.analyze(sentence)[0][0]
        nouns = [token.form for token in tokens if token.tag == "NNG"]

        # ê³ ìœ í•œ í‚¤ë¥¼ ê°€ì§„ ë©€í‹°ì…€ë ‰íŠ¸ ìƒì„±
        selected_nouns_key = f"selected_nouns_{i}"
        selected_nouns = st.multiselect(f'"{sentence}"', nouns, key=selected_nouns_key)

        # ì„ íƒëœ ëª…ì‚¬ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
        st.session_state.selected_nouns_dict[selected_nouns_key] = selected_nouns

        # í˜„ì¬ ë¬¸ì¥ì—ì„œ ì„ íƒëœ ëª…ì‚¬ë¥¼ ë¹ˆì¹¸ìœ¼ë¡œ ì¹˜í™˜
        for noun in selected_nouns:
            sentence = sentence.replace(noun, '___'*len(noun))

        new_sentences.append(sentence)

    # 'ìƒì„±í•˜ê¸°' ë²„íŠ¼
    if st.button('ë¹ˆì¹¸ ëš«ê¸° ë¯¸ë¦¬ë³´ê¸°', key = 'tab1'):
        # ì—…ë°ì´íŠ¸ëœ í…ìŠ¤íŠ¸ í‘œì‹œ
        updated_contents = '\n'.join(new_sentences)
        st.markdown(updated_contents)
        # ì›Œë“œ ë¬¸ì„œ ìƒì„±
        doc = docx.Document()
        doc.add_paragraph(updated_contents)
        
        # ì›Œë“œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        with BytesIO() as f:
            doc.save(f)
            f.seek(0)
            st.download_button('ì›Œë“œ íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œ', f, file_name='í•™ìŠµì§€.docx')

with tab2:
    # í˜•íƒœì†Œ ë¶„ì„
    kiwi = Kiwi()
    tokens = kiwi.analyze(contents)[0][0]

    # ëª…ì‚¬ë¥¼ ì €ì¥í•  ì§‘í•©
    nouns = set()

    # ê° í† í°ì— ëŒ€í•´ ëª…ì‚¬ ì¶”ì¶œ
    for token in tokens:
        if token.tag == "NNG":
            nouns.add(token.form)
    nouns = sorted(nouns)

    # ëª…ì‚¬ ì„ íƒ ìœ„ì ¯
    selected_nouns = st.multiselect('ë¹ˆì¹¸ìœ¼ë¡œ ë§Œë“¤ ëª…ì‚¬ë¥¼ ì„ íƒí•˜ì„¸ìš”', list(nouns))
    # 'ìƒì„±í•˜ê¸°' ë²„íŠ¼
    if st.button('ë¹ˆì¹¸ ëš«ê¸° ë¯¸ë¦¬ë³´ê¸°', key = 'tab2'):
        # ì²´í¬ëœ ëª…ì‚¬ë¥¼ ë¹ˆì¹¸ìœ¼ë¡œ ì¹˜í™˜
        for noun in selected_nouns:
            contents = contents.replace(noun, '___'*len(noun))

        # ê²°ê³¼ í‘œì‹œ
        st.write(contents)

        # ì›Œë“œ ë¬¸ì„œ ìƒì„±
        doc = docx.Document()
        doc.add_paragraph(contents)
        
        # ì›Œë“œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        with BytesIO() as f:
            doc.save(f)
            f.seek(0)
            st.download_button('ì›Œë“œ íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œ', f, file_name='í•™ìŠµì§€.docx')

with tab3:
    st.write("ì¤€ë¹„ ì¤‘ ì…ë‹ˆë‹¤....")
    # # BERT ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
    # tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    # model = BertModel.from_pretrained('bert-base-uncased', output_attentions=True)

    # # ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸
    # text = contents

    # # BERTë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì²˜ë¦¬
    # inputs = tokenizer(text, return_tensors="pt", add_special_tokens=True)
    # outputs = model(**inputs)
    # attentions = outputs.attentions

    # # Attention ê°€ì¤‘ì¹˜ ê³„ì‚° (ë§ˆì§€ë§‰ ë ˆì´ì–´ ì‚¬ìš©)
    # # ì°¨ì› ì¶•ì†Œë¥¼ ìœ„í•´ mean() ëŒ€ì‹  squeeze() ì‚¬ìš©
    # # Attention ê°€ì¤‘ì¹˜ ê³„ì‚° (ë§ˆì§€ë§‰ ë ˆì´ì–´ ì‚¬ìš©)
    # # ì—¬ëŸ¬ ì°¨ì›ì„ ê°€ì§„ í…ì„œë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ mean() ë©”ì†Œë“œ ì‚¬ìš©
    # last_layer_attentions = attentions[-1].squeeze(0)
    # word_attentions = last_layer_attentions.mean(dim=0).squeeze()

    # # ê°€ì¤‘ì¹˜ê°€ ìŠ¤ì¹¼ë¼ê°€ ì•„ë‹ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ì¡°ê±´ë¶€ ì²˜ë¦¬
    # weighted_tokens = []
    # for token, weight in zip(tokens, word_attentions):
    #     if weight.numel() == 1:  # numel() ë©”ì†Œë“œëŠ” í…ì„œ ë‚´ ìš”ì†Œì˜ ì´ ê°œìˆ˜ë¥¼ ë°˜í™˜
    #         weighted_tokens.append((token, weight.item()))
    #     else:
    #         # í…ì„œì— ì—¬ëŸ¬ ìš”ì†Œê°€ ìˆëŠ” ê²½ìš° í‰ê· ê°’ ì‚¬ìš©
    #         weighted_tokens.append((token, weight.mean().item()))


    # # ìƒìœ„ 10ê°œ ì¤‘ìš” ë‹¨ì–´ ì¶”ì¶œ
    # top_tokens = sorted(weighted_tokens, key=lambda x: x[1], reverse=True)[:10]

    # # ì¤‘ìš” ë‹¨ì–´ ì„ íƒ ìœ„ì ¯
    # selected_tokens = st.multiselect('ì¤‘ìš” ë‹¨ì–´ ì„ íƒ', [token for token, weight in top_tokens])

    # # 'ìƒì„±í•˜ê¸°' ë²„íŠ¼
    # if st.button('ë¹ˆì¹¸ ìƒì„±í•˜ê¸°'):
    #     # ì„ íƒëœ í† í°ì„ ë¹ˆì¹¸ìœ¼ë¡œ ì¹˜í™˜
    #     for token in selected_tokens:
    #         text = text.replace(token, "____")
    #     st.write(text)

st.success("ì •ë‹µ íŒŒì¼ ìƒì„± ë° í‚¤ì›Œë“œ ì¶”ì²œ ê¸°ëŠ¥ì€ ì¤€ë¹„ ì¤‘ ì…ë‹ˆë‹¤. ")